# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p_chtY6EfqIsEv_BBtHAg9fVyW3KeyMF
"""

# Install dependencies
!pip install llama-cpp-python==0.2.62 gradio --quiet

# Download the model (you can replace this with DeepSeek or another GGUF model)
!wget -q https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O model.gguf

!pip install llama-cpp-python==0.2.62 gradio --quiet
import gradio as gr
import os
# Import Llama class from llama_cpp
from llama_cpp import Llama  # This line is crucial to fix the error
import contextlib
import os

# Load the model
# Define a custom context manager to suppress stdout/stderr
# This is needed to work around the fileno issue in Jupyter notebooks
# Modified context manager to redirect stdout/stderr to devnull
class suppress_stdout_stderr:
    def __enter__(self):
        self.null_fds = [os.open(os.devnull, os.O_RDWR) for _ in range(2)]
        self.save_fds = [os.dup(1), os.dup(2)]  # Save original stdout/stderr fds
        os.dup2(self.null_fds[0], 1)  # Redirect stdout to devnull
        os.dup2(self.null_fds[1], 2)  # Redirect stderr to devnull

    def __exit__(self, exc_type, exc_val, exc_tb):
        # Restore original stdout/stderr fds
        os.dup2(self.save_fds[0], 1)
        os.dup2(self.save_fds[1], 2)
        os.close(self.null_fds[0])
        os.close(self.null_fds[1])
        os.close(self.save_fds[0])
        os.close(self.save_fds[1])

# Use a context manager that is compatible with Jupyter notebooks
# We use the `redirect_stdout` context manager from `contextlib`
# and redirect both stdout and stderr to /dev/null
with contextlib.redirect_stdout(open(os.devnull, 'w')), contextlib.redirect_stderr(open(os.devnull, 'w')):
    llm = Llama(
        model_path="model.gguf",
        n_ctx=2048,
        n_threads=8,          # Adjust based on your CPU
        use_mlock=True,       # Speeds up on some machines
        verbose=False
    )

# Initialize chat history
chat_history = []

# Format conversation for inference
def format_prompt(history, user_input):
    prompt = ""
    for turn in history:
        prompt += f"User: {turn['user']}\nAssistant: {turn['bot']}\n"
    prompt += f"User: {user_input}\nAssistant:"
    return prompt

# Chat function
def chat_with_model(user_input):
    global chat_history
    prompt = format_prompt(chat_history, user_input)
    output = llm(prompt, max_tokens=512, stop=["User:", "Assistant:"])
    reply = output["choices"][0]["text"].strip()
    chat_history.append({"user": user_input, "bot": reply})
    return reply

# Build Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("# Open Source ChatGPT Clone (Mistral 7B Instruct)")
    chatbot = gr.Chatbot()
    msg = gr.Textbox(placeholder="Ask me anything...", show_label=False)
    clear = gr.Button("Clear Chat")

    def respond(message, chat_ui_history):
        reply = chat_with_model(message)
        chat_ui_history.append((message, reply))
        return chat_ui_history, ""

    def reset_chat():
        global chat_history
        chat_history = []
        return [], ""

    msg.submit(respond, [msg, chatbot], [chatbot, msg])
    clear.click(reset_chat, [], [chatbot, msg])

# Launch the Gradio app
demo.launch(share=True)